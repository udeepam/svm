{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/udeepa/Documents/UCL/Term 2/numerical/svm\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "\n",
    "# from opt.svm import SVC\n",
    "from opt.utils.data_splitter import split4ovr, split4ovo\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:        12665\n",
      "Distribution of training samples:  Counter({1: 6742, 0: 5923})\n",
      "Number of test samples:            2115\n",
      "Distribution of training samples:  Counter({1: 1135, 0: 980})\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = np.load('data/dummy/filtered_mnist.npz')\n",
    "x_train = data['a']\n",
    "y_train = data['b']\n",
    "x_test  = data['c']\n",
    "y_test  = data['d']\n",
    "print(\"Number of training samples:       \", len(y_train))\n",
    "print(\"Distribution of training samples: \", Counter(y_train))\n",
    "print(\"Number of test samples:           \", len(y_test))\n",
    "print(\"Distribution of training samples: \", Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a1a572b00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise data\n",
    "imx, imy = (28,28)\n",
    "visual = np.reshape(x_train[0], (imx,imy))\n",
    "plt.title(\"Example Data Image, y=\"+str(int(y_train[0])))\n",
    "plt.imshow(visual, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "from opt.utils.kernels import polynomial_kernel_matrix, gaussian_kernel_matrix\n",
    "\n",
    "class SVC:\n",
    "    \"\"\"\n",
    "    Multiclass soft-margin kernelised SVM.  \n",
    "    \"\"\"\n",
    "    def __init__(self, C=1.0, kernel=\"gauss\", param=0.5, decision_function_shape=\"ovo\", \n",
    "                 loss_fn='L1', opt_algo=\"smo\"):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        ----------\n",
    "        C : `float`\n",
    "            Regularization parameter. The strength of the regularization is \n",
    "            inversely proportional to C. Must be strictly positive. \n",
    "            The penalty is a squared l2 penalty.\n",
    "        kernel : `str`\n",
    "            The Kernel to use. {'poly', 'gauss'}.\n",
    "        param : `float`\n",
    "            Parameter of the kernel chosen, \n",
    "            i.e. the degree if polynomial or the gamma is gaussian. \n",
    "        decision_function_shape : `str`\n",
    "            The method for multiclass classification. {'ovo','ovr'}.\n",
    "        loss_fn : `str`\n",
    "            The loss function for the optimisation problem. {'L1','L2'}.\n",
    "        opt_algo : `str`\n",
    "            The optimisation method to use. {'barrier', 'smo'}.\n",
    "        \"\"\"         \n",
    "        self.C = float(C)\n",
    "        self.kernel = kernel\n",
    "        self.param = param\n",
    "        self.decision_function_shape = decision_function_shape\n",
    "        self.loss_fn = loss_fn\n",
    "        self.opt_algo = opt_algo\n",
    "        self.classifiers = dict()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Function to train the SVM.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : `dict` of `numpy.ndarray` or `numpy.ndarray`\n",
    "             if self.decision_function_shape=\"ovo\":\n",
    "                 Dictionary of all the kchoose2 datasets for the classifiers.\n",
    "                (nData, nDim) matrix of data.\n",
    "             if self.decision_function_shape=\"ova\"\n",
    "                 (nData, nDim) matrix of data.\n",
    "        y : `dict` of `numpy.ndarray`\n",
    "             if self.decision_function_shape=\"ovo\":\n",
    "                 Dictionary of labels for the kchoose2 classifiers.\n",
    "                (nData,) matrix of corresponding labels. Each element is in the set {-1,+1}.\n",
    "             if self.decision_function_shape=\"ova\"\n",
    "                 Dictionary of labels for the k classifiers.\n",
    "                (nData,) matrix of corresponding labels. Each element is in the set {-1,+1}.\n",
    "        \"\"\"\n",
    "        if self.decision_function_shape == \"ovo\":\n",
    "            for key, val in X.items():\n",
    "                # Get kernel matrix\n",
    "                if self.kernel == \"poly\":\n",
    "                    kernel_martix = polynomial_kernel_matrix(val, val, 0, self.param)\n",
    "                elif self.kernel == \"gauss\":\n",
    "                    kernel_martix = gaussian_kernel_matrix(val, val, self.param)                \n",
    "                self.classifiers[key] = SVM(self.C)\n",
    "                self.classifiers[key].fit(val, y[key], kernel_matrix, self.loss_fn, self.opt_algo)                   \n",
    "                \n",
    "        elif self.decision_function_shape == \"ovr\":\n",
    "            # Get kernel matrix\n",
    "            if self.kernel == \"poly\":\n",
    "                kernel_martix = polynomial_kernel_matrix(X, X, 0, self.param)\n",
    "            elif self.kernel == \"gauss\":\n",
    "                kernel_martix = gaussian_kernel_matrix(X, X, self.param) \n",
    "            for key, val in y.items():\n",
    "                self.classifiers[key] = SVM(self.C)\n",
    "                self.classifiers[key].fit(X, val, kernel_martix, self.loss_fn, self.opt_algo)                             \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict on test set.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : `numpy.ndarray`\n",
    "            (nData, nDim) matrix of test data. Each row corresponds to a data point.  \n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        yhats : `numpy.ndarray`\n",
    "            (nData,) List of predictions on the test data.\n",
    "        \"\"\"        \n",
    "        # Create matrix (nClassifiers, nTestData) \n",
    "        predictions = np.zeros((self.nClassifiers, X.shape[0]))\n",
    "        if self.decision_function_shape == \"ovo\":\n",
    "            for i, (key, val) in enumerate(self.classifiers.items()):\n",
    "                yhats = np.sign(val.predict(X))\n",
    "                # Convert predictions labels to digits.\n",
    "                predictions[i,:] = self.convert_labels2digits(yhats, int(key[0]), int(key[-1]))\n",
    "            # Return the mode label for each test data point.\n",
    "            # If there is more than one such value, only the smallest is returned.\n",
    "            return np.squeeze(stats.mode(predictions, axis=0)[0])\n",
    "       \n",
    "        elif self.decision_function_shape == \"ovr\":\n",
    "            for i, (key, val) in enumerate(self.classifiers.items()):\n",
    "                predictions[i,:] = val.predict(X)\n",
    "            # Return the label with the highest value.    \n",
    "            return np.argmax(predictions, axis=0)            \n",
    "        \n",
    "    @staticmethod\n",
    "    def convert_labels2digits(yhats, pos_label, neg_label):\n",
    "        \"\"\"\n",
    "        Functions that maps +1 to the positive class label \n",
    "        and -1 to the negative class label.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        yhats : `numpy.ndarray`\n",
    "            The predictions from the classifier.\n",
    "        pos_label : `int`\n",
    "            The positive class label.\n",
    "        neg_label : `int`\n",
    "            The negative class label.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        digits : `numpy.ndarray`\n",
    "            The predicitions mapped to the class labels +1 -> pos_labels, -1 -> neg_labels.\n",
    "        \"\"\"\n",
    "        return np.where(yhats==1, pos_label, neg_label)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \"\"\"\n",
    "    Soft-margin kernalised SVM base class\n",
    "    \"\"\"\n",
    "    def __init__(self, C=1.0):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        ----------\n",
    "        C : `float`\n",
    "            Regularization parameter. The strength of the regularization is \n",
    "            inversely proportional to C. Must be strictly positive. \n",
    "            The penalty is a squared l2 penalty.    \n",
    "        \"\"\"        \n",
    "        self.C = float(C)\n",
    "        \n",
    "    def fit(self, X, y, kernel_matrix, loss_fn, opt_algo):\n",
    "        \"\"\"\n",
    "        TODO: Function to fit SVM.\n",
    "        \n",
    "        min 0.5x^TPx + q^Tx\n",
    "        s.t. Gx <= h\n",
    "             Ax = b        \n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : `numpy.ndarray`\n",
    "            (nData, nDim) matrix of data. Each row corresponds to a data point.\n",
    "        y : `numpy.ndarray`\n",
    "            (nData,) matrix of corresponding labels. Each element is in the set {-1,+1}.\n",
    "        kernel_matrix : `numpy.ndarray`\n",
    "            (nData, nData) Kernel matrix of training data.  \n",
    "        loss_fn : `str`\n",
    "            The loss function for the optimisation problem. {'L1','L2'}.            \n",
    "        opt_algo : `str`\n",
    "            The optimisation method to use. {'barrier', 'smo'}.\n",
    "        \"\"\" \n",
    "        info = dict()\n",
    "        \n",
    "        # Convert data to floats\n",
    "        X = X.astype(float)\n",
    "        y = y[:,None].astype(float)\n",
    "        n, d = X.shape\n",
    "        \n",
    "        # Parameters of optimisation problem\n",
    "        if loss_fn == 'L1':\n",
    "            P = y@y.T*kernel_matrix\n",
    "            q = -np.ones((n,1))\n",
    "            G = np.vstack((-np.eye(n), \n",
    "                            np.eye(n)))\n",
    "            h = np.vstack((np.zeros((n,1)), \n",
    "                           self.C*np.ones((n,1))))\n",
    "            A = y.T\n",
    "            b = np.zeros(1)    \n",
    "        elif loss_fn == 'L2':\n",
    "            P = y@y.T*(kernel_matrix + 1/self.C * np.eye(n))\n",
    "            q = -np.ones((n,1))\n",
    "            G = -np.eye(n)\n",
    "            h = np.zeros((n,1))\n",
    "            A = y.T\n",
    "            b = np.zeros(1)  \n",
    "        \n",
    "        # Optimisation        \n",
    "        if opt_algo == \"barrier\":      \n",
    "            # Interior point barrier method specific parameters\n",
    "            x0 = np.zeros((n,1)) # TODO: Need feasible starting point  \n",
    "            t  = 1               # TODO: Choice of t            \n",
    "            mu = 20\n",
    "            tol = 1e-3\n",
    "            max_iter = 100              \n",
    "            # Solve\n",
    "            start_time = time.time()\n",
    "            lambdas, f_min, t, nIter, info = barrier_method(P, q, G, h, A, b, x0, t, mu, tol, max_iter)      \n",
    "            time_taken = time.time()-start_time\n",
    "        elif opt_algo == \"smo\":\n",
    "            # Sequential minimal optimisation method specific parameters\n",
    "            x0 = np.zeros((n,1)) \n",
    "            tol = 1e-3\n",
    "            max_iter = 100               \n",
    "            pass            \n",
    "            \n",
    "        # Support vectors have non-zero lambdas\n",
    "        S  = np.logical_and(lambdas>1e-5, lambdas<=self.C).flatten()\n",
    "        MS = np.logical_and(lambdas>1e-5, lambdas<self.C).flatten()\n",
    "        self.lambdas = lambdas[S]\n",
    "        self.sv   = X[S]\n",
    "        self.sv_y = y[S] \n",
    "        # Get intercept\n",
    "        self.bias = np.mean(y[MS] - ((lambdas[MS]*y[MS]).T@kernel_matrix[MS][:,MS]).T)        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict on test set.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : `numpy.ndarray`\n",
    "            (nData, nDim) matrix of test data. Each row corresponds to a data point.              \n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        yhats : `numpy.ndarray`\n",
    "            (nData,) List of predictions on the test data, not corresponding to labels {-1,+1}\n",
    "            Need to take np.sign() after.\n",
    "        \"\"\"               \n",
    "        # Get kernel matrix between support vectors and test data\n",
    "        if self.kernel == \"poly\":\n",
    "            kernel_matrix = polynomial_kernel_matrix(self.sv, X, 0, self.param)\n",
    "        elif self.kernel == \"gauss\":\n",
    "            kernel_matrix = gaussian_kernel_matrix(self.sv, X, self.param) \n",
    "        return np.squeeze((self.alphas*self.sv_y).T@kernel_matrix + self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation Algorithm 1: Interior Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opt.algos.descent import feasible_newtonLS\n",
    "from opt.algos.linesearch import backtracking\n",
    "from opt.algos.interior_point import FeasibleNewtonCENT\n",
    "\n",
    "def barrier_method(P, q, G, h, A, b, x0, t, mu, tol, max_iter):\n",
    "    \"\"\"\n",
    "    Barrier method \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : `numpy.ndarray`\n",
    "        (d,d) \n",
    "    q : `numpy.ndarray`\n",
    "        (d,1)    \n",
    "    G : `numpy.ndarray`\n",
    "        (m,d) matrix from inequality constraints.\n",
    "    h : `numpy.ndarray`\n",
    "        (m,1)            \n",
    "    A : `numpy.ndarray`\n",
    "        (p,d) matrix from equality constraint. \n",
    "    b : `numpy.ndarray`\n",
    "            (p,1)\n",
    "    x0 : `numpy.ndarray`\n",
    "        Initial iterate.\n",
    "    t : `float`\n",
    "        Parameter for barrier.    \n",
    "    mu : `float`\n",
    "        Increase factor for t.\n",
    "    tol : `float`\n",
    "        tolerance on the (duality gap ~ m/t)\n",
    "    max_iter : `int`\n",
    "        Maximum number of iterations.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    x_min : `numpy.ndarray`\n",
    "        Minimum of objective function F\n",
    "    f_min : `float`\n",
    "        Value of minimum objective function F\n",
    "    nIter : `int`\n",
    "        Number of iterations.\n",
    "    info : `dict` of `list`\n",
    "        Information about iteration.\n",
    "    \"\"\"\n",
    "    # Initialisation\n",
    "    nIter = 0\n",
    "    stopCond = False\n",
    "    x_k = x0\n",
    "    F = FeasibleNewtonCENT(P, q, G, h, A, b, t)\n",
    "    \n",
    "    print(\"F.F.f:   \", F.F.f(x0).shape)\n",
    "    print(\"F.F.df:  \", F.F.df(x0).shape)\n",
    "    print(\"F.F.d2f: \", F.F.d2f(x0).shape)\n",
    "    print(\"F.phi.f:   \", F.phi.f(x0).shape)\n",
    "    print(F.phi.f(x0))\n",
    "    print(\"F.phi.df:  \", F.phi.df(x0).shape)\n",
    "#     print(\"F.phi.d2f: \", F.phi.d2f(x0).shape)\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    info = defaultdict(list)\n",
    "    info['xs'].append(x_k)\n",
    "    info['fs'].append(F.f(x_k))\n",
    "    info['inIter'].append(0)\n",
    "    info['dGap'].append(1/t)\n",
    "    \n",
    "    # Parameters for centering step\n",
    "    m = G.shape[0] \n",
    "    alpha0 =1\n",
    "    c1  = 1e-4\n",
    "    c2  = 0.9\n",
    "    rho = 0.5\n",
    "    tolNewton = 1e-12\n",
    "    maxIterNewton = 100\n",
    "    \n",
    "    while stopCond is False and nIter < max_iter:\n",
    "        # Create function handler for centering step \n",
    "        # (needs to be redefined at each step because of changing \"t\")\n",
    "        F = FeasibleNewtonCENT(P, q, G, h, A, b, t)\n",
    "        \n",
    "        # Line search function (needs to be redefined at each step because of changing F) \n",
    "        lsFun = lambda F, x_k, p_k, alpha0: backtracking(F, x_k, p_k, alpha0, rho, c1)\n",
    "        # Centering step\n",
    "        x_k, f_k, nIterLS, infoLS = feasible_newtonLS(F, lsFun, alpha0, x_k, tolNewton, maxIterNewton)   \n",
    "        \n",
    "        # Check stopping condition (m/t).\n",
    "        if m/t < tol: \n",
    "            stopCond = True\n",
    "\n",
    "        # Increase t\n",
    "        t *= mu\n",
    "\n",
    "        # Store info\n",
    "        info['xs'].append(x_k)\n",
    "        info['fs'].append(f_k)\n",
    "        info['inIter'].append(nIterLS)\n",
    "        info['dGap'].append(1/t)\n",
    "\n",
    "        # Increment number of iterations\n",
    "        nIter += 1\n",
    "    \n",
    "    return x_k, F.f(x_k), t, nIter, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feasible_newtonLS(F, ls, alpha0, x0, tol, max_iter):\n",
    "    \"\"\"\n",
    "    Feasible Newton method: Newton method which starts at a feasible point \n",
    "    and subsequently enforces the equality constraints on the step maintaining feasibility.\n",
    "    Reference: Algorithm 10.1 from Boyd, Convex Optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    F : `function`\n",
    "        Objective function.\n",
    "    ls : `function`\n",
    "        Specifies line search algorithm.\n",
    "    alpha0 : `float`\n",
    "        initial step length.\n",
    "    x0 : `numpy.ndarray`\n",
    "        (d,1) Feasible initial iterate.\n",
    "    tol : `float`\n",
    "        Stopping condition on minimal allowed step.\n",
    "    max_iter : `int`\n",
    "        Maximum number of iterations.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    x_min : `numpy.ndarray`\n",
    "        Minimum of objective function F\n",
    "    f_min : `float`\n",
    "        Value of minimum objective function F\n",
    "    nIter : `int`\n",
    "        Number of iterations.\n",
    "    info : `dict` of `list`\n",
    "        Information about iteration.       \n",
    "    \"\"\"  \n",
    "    # Initialisation\n",
    "    nIter = 0\n",
    "    x_k = x0\n",
    "    d = x0.shape[0]\n",
    "    info = defaultdict(list)\n",
    "    info['xs'].append(x0)\n",
    "    info['alphas'].append(alpha0)   \n",
    "    \n",
    "    # Loop until convergence or maximum number of iterations\n",
    "    while True:\n",
    "        # Increment iterations\n",
    "        nIter += 1\n",
    "        \n",
    "        # Compute descent direction\n",
    "        p_k = -np.linalg.pinv(F.d2f(x_k))@F.df(x_k) # Newton direction\n",
    "        if p_k.T@F.df(x_k)>0:\n",
    "            # Force to be descent direction (only active if F.d2f(x_k) not pos.def.)\n",
    "            p_k = -p_k       \n",
    "        # Get Newton step\n",
    "        delta_x_k = p_k[:d,:]\n",
    "        \n",
    "        # Stopping condition\n",
    "        # Compute Newton decrement\n",
    "        lambda_k = (-delta_x_k.T@F.d2f(x_k)[:d,:d]@delta_x_k)**0.5\n",
    "        if 0.5*lambda_k**(2) <= tol:\n",
    "            break     \n",
    "            \n",
    "        # Call line search given by handle ls for computing step length\n",
    "        alpha_k = ls(F, x_k, delta_x_k, alpha0)            \n",
    "        # Update x_k\n",
    "        x_k = x_k + alpha_k*delta_x_k               \n",
    "        \n",
    "        # Store iteration info\n",
    "        info['xs'].append(x_k)\n",
    "        info['alphas'].append(alpha_k)\n",
    "            \n",
    "    return x_k, F.f(x_k), nIter, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation Algorithm 2: SMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation Algorithm 3: Coordinate Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F.F.f:    ()\n",
      "F.F.df:   (12665, 1)\n",
      "F.F.d2f:  (12665, 12665)\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "F.phi.f:    ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/udeepa/Documents/UCL/Term 2/numerical/svm/opt/algos/interior_point.py:202: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(np.log(self.h - self.G@x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/udeepa/Documents/UCL/Term 2/numerical/svm/opt/algos/interior_point.py:216: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  tmp = 1 / (self.h - self.G@x)\n",
      "/Users/udeepa/Documents/UCL/Term 2/numerical/svm/opt/algos/interior_point.py:217: RuntimeWarning: invalid value encountered in matmul\n",
      "  return self.G.T@tmp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F.phi.df:   (12665, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-71675e638dbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Train: OVR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0my_train_ovr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit4ovr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_ovr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# # Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f670f7051bd9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_martix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_algo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-eb2d31cb2098>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, kernel_matrix, loss_fn, opt_algo)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Solve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnIter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbarrier_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mopt_algo\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"smo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "svm = SVC(C=1.0, \n",
    "          kernel=\"gauss\", \n",
    "          param=0.5, \n",
    "          decision_function_shape=\"ovr\",\n",
    "          loss_fn='L1',\n",
    "          opt_algo=\"barrier\")\n",
    "\n",
    "# Train: OVO\n",
    "x_train_ovo, y_train_ovo = split4ovo(x_train, y_train)\n",
    "# svm.fit(x_train_ovo, y_train_ovo)\n",
    "\n",
    "# Train: OVR\n",
    "y_train_ovr = split4ovr(y_train)\n",
    "svm.fit(x_train, y_train_ovr)\n",
    "\n",
    "# # Test\n",
    "# yhat = svm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train.astype(float)\n",
    "y = y_train_ovr[\"0\"][:,None].astype(float)\n",
    "n, d = X.shape\n",
    "kernel_matrix = gaussian_kernel_matrix(X, X, 0.5)   \n",
    "C = 1\n",
    "x0 = np.zeros((n,1))\n",
    "\n",
    "P = y@y.T*kernel_matrix\n",
    "q = -np.ones((n,1))\n",
    "G = np.vstack((-np.eye(n), \n",
    "                np.eye(n)))\n",
    "h = np.vstack((np.zeros((n,1)), \n",
    "               C*np.ones((n,1))))\n",
    "A = y.T\n",
    "b = np.zeros(1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.log(h - G@x0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
